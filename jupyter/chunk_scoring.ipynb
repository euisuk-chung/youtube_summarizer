{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.neural import MultiHeadedAttention, PositionwiseFeedForward\n",
    "from models.model_builder import Bert\n",
    "from models.encoder import Classifier, PositionalEncoding, TransformerEncoderLayer, ExtTransformerEncoder\n",
    "from models.trainer_ext import Trainer\n",
    "from models.data_loader import TextLoader, load_dataset, Dataloader, get_kobert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "    \"visible_gpus\" : -1,\n",
    "    \"temp_dir\" : './tmp/',\n",
    "    \"test_from\": None,\n",
    "    \"max_pos\" : 512,\n",
    "    \"large\" : False,\n",
    "    \"finetune_bert\": True,\n",
    "    \"encoder\": \"bert\",\n",
    "    \"share_emb\": False,\n",
    "    \"dec_layers\": 6,\n",
    "    \"dec_dropout\": 0.2,\n",
    "    \"dec_hidden_size\": 768,\n",
    "    \"dec_heads\": 8,\n",
    "    \"dec_ff_size\": 2048,\n",
    "    \"enc_hidden_size\": 512,\n",
    "    \"enc_ff_size\": 512,\n",
    "    \"enc_dropout\": 0.2,\n",
    "    \"enc_layers\": 6,\n",
    "    \n",
    "    \"ext_dropout\": 0.2,\n",
    "    \"ext_layers\": 2,\n",
    "    \"ext_hidden_size\": 768,\n",
    "    \"ext_heads\": 8,\n",
    "    \"ext_ff_size\": 2048,\n",
    "    \n",
    "    \"accum_count\": 1,\n",
    "    \"save_checkpoint_steps\": 5,\n",
    "    \n",
    "    \"generator_shard_size\": 32,\n",
    "    \"alpha\": 0.6,\n",
    "    \"beam_size\": 5,\n",
    "    \"min_length\": 15,\n",
    "    \"max_length\": 150,\n",
    "    \"max_tgt_len\": 140,  \n",
    "    \"block_trigram\": True,\n",
    "    \n",
    "    \"model_path\": \"./tmp_model/\",\n",
    "    \"result_path\": \"./tmp_result/src\",\n",
    "    \"recall_eval\": False,\n",
    "    \"report_every\": 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, heads, dropout, num_inter_layers=0):\n",
    "        super(ExtTransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_inter_layers = num_inter_layers\n",
    "        self.pos_emb = PositionalEncoding(dropout, d_model)\n",
    "        self.transformer_inter = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(d_model, heads, d_ff, dropout)\n",
    "             for _ in range(num_inter_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.wo = nn.Linear(d_model, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, top_vecs, mask):\n",
    "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
    "\n",
    "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
    "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
    "        x = top_vecs * mask[:, :, None].float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for i in range(self.num_inter_layers):\n",
    "            x = self.transformer_inter[i](i, x, x, 1 - mask)  # all_sents * max_tokens * dim\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        sent_scores = self.sigmoid(self.wo(x))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "\n",
    "        return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtSummarizer(nn.Module):\n",
    "    def __init__(self, args, device, checkpoint):\n",
    "        super(ExtSummarizer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.bert = Bert(args.large, args.temp_dir, args.finetune_bert)\n",
    "        self.ext_layer = ExtTransformerEncoder(self.bert.model.config.hidden_size,\\\n",
    "                                                args.ext_ff_size, args.ext_heads,\\\n",
    "                                                args.ext_dropout, args.ext_layers)\n",
    "        \n",
    "        if (args.encoder == 'baseline'):\n",
    "            bert_config = BertConfig(self.bert.model.config.vocab_size, hidden_size=args.ext_hidden_size,\n",
    "                                     num_hidden_layers=args.ext_layers, num_attention_heads=args.ext_heads, intermediate_size=args.ext_ff_size)\n",
    "            self.bert.model = BertModel(bert_config)\n",
    "            self.ext_layer = Classifier(self.bert.model.config.hidden_size)\n",
    "\n",
    "        if(args.max_pos>512):\n",
    "            my_pos_embeddings = nn.Embedding(args.max_pos, self.bert.model.config.hidden_size)\n",
    "            my_pos_embeddings.weight.data[:512] = self.bert.model.embeddings.position_embeddings.weight.data\n",
    "            my_pos_embeddings.weight.data[512:] = self.bert.model.embeddings.position_embeddings.weight.data[-1][None,:].repeat(args.max_pos-512,1)\n",
    "            self.bert.model.embeddings.position_embeddings = my_pos_embeddings\n",
    "\n",
    "\n",
    "        if checkpoint is not None:\n",
    "            self.load_state_dict(checkpoint['model'], strict=True)\n",
    "        else:\n",
    "            if args.param_init != 0.0:\n",
    "                for p in self.ext_layer.parameters():\n",
    "                    p.data.uniform_(-args.param_init, args.param_init)\n",
    "            if args.param_init_glorot:\n",
    "                for p in self.ext_layer.parameters():\n",
    "                    if p.dim() > 1:\n",
    "                        xavier_uniform_(p)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, src, segs, clss, mask_src, mask_cls):\n",
    "        top_vec = self.bert(src, segs, mask_src)\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        #sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
    "        return sents_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ExtSummarizer(\n",
       "  (bert): Bert(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(8004, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ext_layer): ExtTransformerEncoder(\n",
       "    (pos_emb): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.2)\n",
       "    )\n",
       "    (transformer_inter): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (softmax): Softmax()\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout_1): Dropout(p=0.2)\n",
       "          (dropout_2): Dropout(p=0.2)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (softmax): Softmax()\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout_1): Dropout(p=0.2)\n",
       "          (dropout_2): Dropout(p=0.2)\n",
       "        )\n",
       "        (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
       "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "device = \"cpu\" if args.visible_gpus == -1 else \"cuda\"\n",
    "checkpoint = torch.load('./checkpoint/model_step_24000.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "model = ExtSummarizer(args, device, checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance # for measuring distance\n",
    "\n",
    "class WindowEmbedder:\n",
    "    \n",
    "    def __init__(self, src_doc='', \\\n",
    "                window_size=3, text_loader=None, \\\n",
    "                agg_mode='mean', dist_mode='cosine'):\n",
    "\n",
    "        self.src_doc = src_doc\n",
    "        self.window_size = window_size\n",
    "        self.sent_list = [sent for sent in src_doc.split('\\n') if sent]\n",
    "        self.text_loader = text_loader\n",
    "        self.agg_mode = agg_mode\n",
    "        self.dist_mode = dist_mode\n",
    "    \n",
    "    \n",
    "    def get_cand_divpoints(self):\n",
    "        \n",
    "        window_size = self.window_size\n",
    "        sent_len = len(self.sent_list)\n",
    "        div_cands = list(np.arange(window_size-1, sent_len-window_size))\n",
    "        return div_cands\n",
    "\n",
    "    \n",
    "    def embedder(self, target_doc=None):\n",
    "\n",
    "        batch_iter = self.text_loader.load_text(target_doc, '\\n')\n",
    "        for _, batch in enumerate(batch_iter):\n",
    "            src = batch.src\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask, mask_cls = batch.mask_src, batch.mask_cls\n",
    "            result_vec = model(src, segs, clss, mask, mask_cls).detach()\n",
    "        return result_vec\n",
    "    \n",
    "    \n",
    "    def get_embeddings(self):\n",
    "\n",
    "        div_cands = self.get_cand_divpoints()\n",
    "        embedded_result = []\n",
    "        for d in div_cands:\n",
    "            min_pos = d - self.window_size + 1\n",
    "            max_pos = d + self.window_size\n",
    "        \n",
    "            target_sents = self.sent_list[min_pos:max_pos+1]\n",
    "            target_doc = '\\n'.join((target_sents))\n",
    "            tmp_embedded = self.embedder(target_doc=target_doc)\n",
    "            embedded_result.append(tmp_embedded.squeeze(0))\n",
    "        return embedded_result\n",
    "    \n",
    "    def agg_vectors(self, vec1=None, vec2=None, mode = 'mean'):\n",
    "\n",
    "        if mode=='mean':\n",
    "            vec1_agg = torch.mean(vec1, dim=0)\n",
    "            vec2_agg = torch.mean(vec2, dim=0)\n",
    "\n",
    "        elif mode=='max':\n",
    "            vec1_agg = torch.max(vec1, dim=0).values\n",
    "            vec2_agg = torch.max(vec2, dim=0).values\n",
    "        \n",
    "        elif mode=='min':\n",
    "            vec1_agg = torch.min(vec1, dim=0).values\n",
    "            vec2_agg = torch.min(vec2, dim=0).values\n",
    "\n",
    "        elif mode=='max_min':\n",
    "            vec1_max = torch.max(vec1, dim=0).values\n",
    "            vec1_min = torch.min(vec1, dim=0).values\n",
    "            vec2_max = torch.max(vec2, dim=0).values\n",
    "            vec2_min = torch.min(vec2, dim=0).values\n",
    "            vec1_agg = torch.cat((vec1_max, vec1_min))\n",
    "            vec2_agg = torch.cat((vec2_max, vec2_min))\n",
    "\n",
    "        return vec1_agg, vec2_agg\n",
    "\n",
    "    \n",
    "    def dist_calculator(self, vec1, vec2, mode = 'cosine'):\n",
    "        if mode == 'euclidean':\n",
    "            diff = distance.euclidean(vec1, vec2)\n",
    "        elif mode =='cosine':\n",
    "            diff = distance.cosine(vec1, vec2)\n",
    "        elif mode == 'jensenshannon':\n",
    "            diff = distance.jensenshannon(vec1, vec2)\n",
    "        return diff\n",
    "    \n",
    "    \n",
    "    def detect_divpoints(self):\n",
    "        \n",
    "        embedded_result = self.get_embeddings()\n",
    "        \n",
    "        dist_result = []\n",
    "        for emb in embedded_result:\n",
    "            lh = emb[:self.window_size]\n",
    "            rh = emb[self.window_size:]\n",
    "            \n",
    "            lh_agg, rh_agg = self.agg_vectors(lh, rh, mode=self.agg_mode)\n",
    "            dist = self.dist_calculator(lh_agg, rh_agg, mode=self.dist_mode)\n",
    "            dist_result.append(dist)\n",
    "            #dist_result.append(dist.item())\n",
    "        \n",
    "        return dist_result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기사로 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 260697 records from ../../data/news/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "news_df = load_jsonl('../../data/news/train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "# (1) 글자 개수가 너무 작은 경우 없애기 (30글자 이상)\n",
    "# (2) 문장이 적은 경우 해당 기사 없애기 (10문장 이상)\n",
    "news_clean = []\n",
    "for news in news_df:\n",
    "    news_article = news['article_original']\n",
    "    if len(news_article) >= 10:\n",
    "        article_clean = [sent for sent in news_article if len(sent) >= 30]\n",
    "        news_clean.append(article_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_mixed_doc(news_dataset=None, max_num=1000):\n",
    "    mixed_doc_set = []\n",
    "    for i in range(max_num):\n",
    "        lh_count = min(random.randint(7, 10), len(news_dataset[i]))\n",
    "        rh_count = min(random.randint(7, 10), len(news_dataset[i+1]))\n",
    "\n",
    "        lh_news = news_dataset[i][:lh_count]\n",
    "        rh_news = news_dataset[i+1][:rh_count]\n",
    "        \n",
    "        gt = lh_count - 1\n",
    "\n",
    "        src_doc = '\\n'.join((lh_news + rh_news))\n",
    "        mixed_doc_set.append((src_doc, gt))\n",
    "        \n",
    "    return mixed_doc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2020011135)\n",
    "mixed_doc_list = make_mixed_doc(news_dataset=news_clean, max_num=500) # previous 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "len(mixed_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using cached model\nusing cached model\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "loader = TextLoader(args, device)\n",
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('min', 'euclidean'),\n",
       " ('min', 'cosine'),\n",
       " ('min', 'jensenshannon'),\n",
       " ('max', 'euclidean'),\n",
       " ('max', 'cosine'),\n",
       " ('max', 'jensenshannon'),\n",
       " ('mean', 'euclidean'),\n",
       " ('mean', 'cosine'),\n",
       " ('mean', 'jensenshannon'),\n",
       " ('max_min', 'euclidean'),\n",
       " ('max_min', 'cosine'),\n",
       " ('max_min', 'jensenshannon')]"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "## make comparison tuples\n",
    "arguments = [('min','euclidean'), ('min','cosine'), ('min','jensenshannon'),\\\n",
    "            ('max','euclidean'), ('max','cosine'), ('max','jensenshannon'),\\\n",
    "            ('mean','euclidean'), ('mean','cosine'), ('mean','jensenshannon'),\\\n",
    "            ('max_min','euclidean'), ('max_min','cosine'), ('max_min','jensenshannon')]\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ACC', 'div_result']"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "## make comparison tuples\n",
    "sub_arguments = ['ACC', 'div_result']\n",
    "sub_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ACC': None, 'div_result': None}"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "sub_args_dict = {key: None for key in sub_arguments}\n",
    "sub_args_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{('min', 'euclidean'): {'ACC': None, 'div_result': None},\n",
       " ('min', 'cosine'): {'ACC': None, 'div_result': None},\n",
       " ('min', 'jensenshannon'): {'ACC': None, 'div_result': None},\n",
       " ('max', 'euclidean'): {'ACC': None, 'div_result': None},\n",
       " ('max', 'cosine'): {'ACC': None, 'div_result': None},\n",
       " ('max', 'jensenshannon'): {'ACC': None, 'div_result': None},\n",
       " ('mean', 'euclidean'): {'ACC': None, 'div_result': None},\n",
       " ('mean', 'cosine'): {'ACC': None, 'div_result': None},\n",
       " ('mean', 'jensenshannon'): {'ACC': None, 'div_result': None},\n",
       " ('max_min', 'euclidean'): {'ACC': None, 'div_result': None},\n",
       " ('max_min', 'cosine'): {'ACC': None, 'div_result': None},\n",
       " ('max_min', 'jensenshannon'): {'ACC': None, 'div_result': None}}"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "result_dict = {key: sub_args_dict for key in arguments}\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('min', 'euclidean')\n>>>>{'ACC': None, 'div_result': None}\n('min', 'cosine')\n>>>>{'ACC': None, 'div_result': None}\n('min', 'jensenshannon')\n>>>>{'ACC': None, 'div_result': None}\n('max', 'euclidean')\n>>>>{'ACC': None, 'div_result': None}\n('max', 'cosine')\n>>>>{'ACC': None, 'div_result': None}\n('max', 'jensenshannon')\n>>>>{'ACC': None, 'div_result': None}\n('mean', 'euclidean')\n>>>>{'ACC': None, 'div_result': None}\n('mean', 'cosine')\n>>>>{'ACC': None, 'div_result': None}\n('mean', 'jensenshannon')\n>>>>{'ACC': None, 'div_result': None}\n('max_min', 'euclidean')\n>>>>{'ACC': None, 'div_result': None}\n('max_min', 'cosine')\n>>>>{'ACC': None, 'div_result': None}\n('max_min', 'jensenshannon')\n>>>>{'ACC': None, 'div_result': None}\n"
     ]
    }
   ],
   "source": [
    "for argument in list(result_dict.keys()):\n",
    "    print(argument)\n",
    "    print(f'>>>>{result_dict[argument]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl\n",
    "# import os \n",
    "\n",
    "# save_path = \"./chunk_results\"\n",
    "\n",
    "# with open(os.path.join(save_path, \"chunk_results.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('min', 'euclidean')\n('min', 'cosine')\n('min', 'jensenshannon')\n('max', 'euclidean')\n('max', 'cosine')\n('max', 'jensenshannon')\n('mean', 'euclidean')\n('mean', 'cosine')\n('mean', 'jensenshannon')\n('max_min', 'euclidean')\n('max_min', 'cosine')\n('max_min', 'jensenshannon')\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('min', 'euclidean'),\n",
       " ('min', 'cosine'),\n",
       " ('min', 'jensenshannon'),\n",
       " ('max', 'euclidean'),\n",
       " ('max', 'cosine'),\n",
       " ('max', 'jensenshannon'),\n",
       " ('mean', 'euclidean'),\n",
       " ('mean', 'cosine'),\n",
       " ('mean', 'jensenshannon'),\n",
       " ('max_min', 'euclidean'),\n",
       " ('max_min', 'cosine'),\n",
       " ('max_min', 'jensenshannon')]"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "done_list = []\n",
    "for argument in arguments : \n",
    "    print(argument)\n",
    "    done_list.append(argument)\n",
    "done_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running min, euclidean experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('min', 'euclidean') accuracy : 14.000000000000002\n",
      "Running min, cosine experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('min', 'cosine') accuracy : 15.8\n",
      "Running min, jensenshannon experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('min', 'jensenshannon') accuracy : 0.0\n",
      "Running max, euclidean experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('max', 'euclidean') accuracy : 12.0\n",
      "Running max, cosine experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('max', 'cosine') accuracy : 13.0\n",
      "Running max, jensenshannon experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('max', 'jensenshannon') accuracy : 0.0\n",
      "Running mean, euclidean experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('mean', 'euclidean') accuracy : 21.2\n",
      "Running mean, cosine experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('mean', 'cosine') accuracy : 16.2\n",
      "Running mean, jensenshannon experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('mean', 'jensenshannon') accuracy : 0.0\n",
      "Running max_min, euclidean experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('max_min', 'euclidean') accuracy : 12.6\n",
      "Running max_min, cosine experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('max_min', 'cosine') accuracy : 14.399999999999999\n",
      "Running max_min, jensenshannon experiment : \n",
      "working on 20th doc...\n",
      "working on 40th doc...\n",
      "working on 60th doc...\n",
      "working on 80th doc...\n",
      "working on 100th doc...\n",
      "working on 120th doc...\n",
      "working on 140th doc...\n",
      "working on 160th doc...\n",
      "working on 180th doc...\n",
      "working on 200th doc...\n",
      "working on 220th doc...\n",
      "working on 240th doc...\n",
      "working on 260th doc...\n",
      "working on 280th doc...\n",
      "working on 300th doc...\n",
      "working on 320th doc...\n",
      "working on 340th doc...\n",
      "working on 360th doc...\n",
      "working on 380th doc...\n",
      "working on 400th doc...\n",
      "working on 420th doc...\n",
      "working on 440th doc...\n",
      "working on 460th doc...\n",
      "working on 480th doc...\n",
      "working on 500th doc...\n",
      "('max_min', 'jensenshannon') accuracy : 0.0\n"
     ]
    }
   ],
   "source": [
    "# save as pkl\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "save_path = \"./chunk_result\"\n",
    "\n",
    "# 학습 완료된 model param 추적\n",
    "if \"result_log.pkl\" in os.listdir(save_path):\n",
    "    with open(os.path.join(save_path, \"result_log.pkl\"), \"rb\") as f1:\n",
    "        done_list = pickle.load(f1)\n",
    "else:\n",
    "    done_list = []\n",
    "\n",
    "# 학습 중 튕겼을 시를 대비한 코드\n",
    "if \"chunk_results.pkl\" in os.listdir(save_path):\n",
    "    with open(os.path.join(save_path, \"chunk_results.pkl\"), \"rb\") as f2:\n",
    "        result_dict = pickle.load(f2)\n",
    "else:\n",
    "    arguments = [('min','euclidean'), ('min','cosine'), ('min','jensenshannon'),\\\n",
    "            ('max','euclidean'), ('max','cosine'), ('max','jensenshannon'),\\\n",
    "            ('mean','euclidean'), ('mean','cosine'), ('mean','jensenshannon'),\\\n",
    "            ('max_min','euclidean'), ('max_min','cosine'), ('max_min','jensenshannon')]\n",
    "\n",
    "    sub_arguments = ['ACC', 'div_result']\n",
    "\n",
    "    sub_args_dict = {key: None for key in sub_arguments}\n",
    "    result_dict = {key: sub_args_dict for key in arguments}\n",
    "\n",
    "\n",
    "# inference\n",
    "for argument in list(result_dict.keys()):\n",
    "\n",
    "    if argument in done_list:\n",
    "        # 이미 학습된 것이 있으면 SKIP\n",
    "        print(f\"[{argument}] results already exist ! -- skipping inference\")\n",
    "        continue\n",
    "    else:\n",
    "        # 이미 학습된 것이 있으면 학습 수행\n",
    "        agg_mode, dist_mode = argument\n",
    "        \n",
    "        print(f\"Running {agg_mode}, {dist_mode} experiment : \")\n",
    "\n",
    "        err_cnt = 0\n",
    "        acc_cnt = 0\n",
    "        div_result = []\n",
    "\n",
    "        for i, a_set in enumerate(mixed_doc_list):\n",
    "            \n",
    "            if (i+1) % 20 == 0:\n",
    "                print(f\"working on {i+1}th doc...\")\n",
    "                \n",
    "            src_doc = a_set[0]\n",
    "            gt = a_set[1]\n",
    "            \n",
    "            window_embedder = WindowEmbedder(src_doc=src_doc, window_size=window_size, text_loader=loader, agg_mode=agg_mode, dist_mode=dist_mode)\n",
    "            div_scores = window_embedder.detect_divpoints()\n",
    "            div_point = (window_size - 1) + div_scores.index(max(div_scores))\n",
    "\n",
    "            if div_point == gt:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "            \n",
    "            sents = [sent for sent in src_doc.split('\\n') if sent]\n",
    "            lh_sent, rh_sent = [], []\n",
    "            for i, sent in enumerate(sents):\n",
    "                if i <= div_point:\n",
    "                    lh_sent.append(sent)\n",
    "                else:\n",
    "                    rh_sent.append(sent)\n",
    "                    \n",
    "            result_sents = lh_sent + [\"----------------[DIV]---------------\"] + rh_sent\n",
    "            div_result.append((result_sents, div_scores, div_point, gt))     \n",
    "\n",
    "        # get acc for final argument\n",
    "        acc = acc_cnt/(acc_cnt + err_cnt)*100\n",
    "        result_dict[argument]['ACC'] = acc\n",
    "        print(f'{argument} accuracy : {acc}')\n",
    "\n",
    "        # save div_results for final argument\n",
    "        result_dict[argument]['div_result'] = div_result\n",
    "\n",
    "        # check if it is ran well\n",
    "        # print(result_dict)\n",
    "\n",
    "        # add to done list\n",
    "        done_list.append(argument)\n",
    "\n",
    "        # save result\n",
    "        with open(os.path.join(save_path, \"result_log.pkl\"), \"wb\") as f3:\n",
    "            pickle.dump(done_list, f3)\n",
    "\n",
    "        with open(os.path.join(save_path, \"chunk_results.pkl\"), \"wb\") as f4:\n",
    "            pickle.dump(result_dict, f4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for key,value in result_dict.items():\n",
    "    result_df = pd.DataFrame(value, columns=['ACC', 'div_result'])\n",
    "    result_df.to_csv(f'/repo/course/sem21_01/youtube-summarization/src/bertsum/chunk_result/{key}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in result_dict.items():\n",
    "    if key == ('mean', 'jensenshannon'):\n",
    "        mean_js = value\n",
    "    elif  key == ('mean', 'cosine'):\n",
    "        mean_cos = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(mean_js, columns=['ACC', 'div_result'])\n",
    "result_df.to_csv('mean_js.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   ACC                                         div_result\n0  0.0  ([지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 ...\n"
     ]
    }
   ],
   "source": [
    "print(result_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(mean_cos, columns=['ACC', 'div_result'])\n",
    "result_df.to_csv('mean_cos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   ACC                                         div_result\n0  0.0  ([지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 ...\n"
     ]
    }
   ],
   "source": [
    "print(result_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 미수행 실험건\n",
    "- 추가 word embedding 건에 대해\n",
    "- 각각 window 별로 다른 bert에 넣어서 비교"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python369jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}